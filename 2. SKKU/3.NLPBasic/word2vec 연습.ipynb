{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "from gensim.models import Word2Vec\n",
    "import sys\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'GoogleNews-vectors-negative300.bin'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = KeyedVectors.load_word2vec_format(filename, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('queen', 0.7118191719055176)]\n"
     ]
    }
   ],
   "source": [
    "result = model.most_similar(positive=['woman', 'king'], negative=['man'], topn=1)\n",
    "print(result)\n",
    "\n",
    "result = model.most_similar(positive=['woman',  'king'], negative=['man'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method WordEmbeddingsKeyedVectors.accuracy of <gensim.models.keyedvectors.Word2VecKeyedVectors object at 0x7f52c15b2d30>>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = [['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec'],\n",
    "\t\t\t['this', 'is', 'the', 'second', 'sentence'],\n",
    "\t\t\t['yet', 'another', 'sentence'],\n",
    "\t\t\t['one', 'more', 'sentence'],\n",
    "\t\t\t['and', 'the', 'final', 'sentence']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'the', 'first', 'sentence', 'for', 'word2vec', 'second', 'yet', 'another', 'one', 'more', 'and', 'final']\n",
      "[('yet', 0.04476505517959595)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/etc/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:16: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  app.launch_new_instance()\n"
     ]
    }
   ],
   "source": [
    "# train model\n",
    "model = Word2Vec(sentences, min_count=1)\n",
    "# summarize vocabulary\n",
    "words = list(model.wv.vocab)\n",
    "print(words)\n",
    "# access vector for one word\n",
    "#print(model['sentence'])\n",
    "# save model\n",
    "model.save('model.bin')\n",
    "# load model\n",
    "new_model = Word2Vec.load('model.bin')\n",
    "# summarize the loaded model\n",
    "# print(new_model)\n",
    "\n",
    "#result = model.most_similar(positive=['this', 'is'], negative=['the'], topn=1)\n",
    "result = model.most_similar(positive=['this', 'is', 'the'], topn=1)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#with open('lalaland.txt') as f:\n",
    "#    raw1 = f.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import PlaintextCorpusReader\n",
    "lalaland = PlaintextCorpusReader('.','godfather.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "lalaland_words = lalaland.words()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "join_lalaland_words = ' '.join(lalaland_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "190132"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = open(\"prep_godfather.txt\",'a')\n",
    "f.write(join_lalaland_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "import random\n",
    "lalaland = PlaintextCorpusReader('.','prep_lalaland.txt')\n",
    "godfater = PlaintextCorpusReader('.','prep_godfather.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['HE', 'GODFATHER', '_____________', 'Screenplay', 'by', 'MARIO', 'PUZO', 'and', 'FRANCIS', 'FORD', 'COPPOLA', 'THIRD', 'DRAFT', 'PARAMOUNT', 'PICTURES', '1', 'Gulf', 'and', 'Western', 'Plaza', 'March', '29', ',', '1971', 'New', 'York', ',', 'New', 'York', '10019', 'INT', 'DAY', ':', 'DON', \"'\", 'S', 'OFFICE', '(', 'SUMMER', '1945', ')', 'The', 'PARAMOUNT', 'Logo', 'is', 'presented', 'austerely', 'over', 'a', 'black', 'background', '.'], ['There', 'is', 'a', 'moment', \"'\", 's', 'hesitation', ',', 'and', 'then', 'the', 'simple', 'words', 'in', 'white', 'lettering', ':', 'THE', 'GODFATHER', 'While', 'this', 'remains', ',', 'we', 'hear', ':', '\"', 'I', 'believe', 'in', 'America', '.\"'], ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lalaland.sents()\n",
    "godfater.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "test_label = []\n",
    "for w in lalaland.sents():\n",
    "    test_data.append(w)\n",
    "    test_label.append('lala')\n",
    "    \n",
    "for w in godfater.sents():\n",
    "    test_data.append(w)\n",
    "    test_label.append('god')\n",
    "    \n",
    "c = list(zip(test_data, test_label))\n",
    "\n",
    "#random.shuffle(c)\n",
    "\n",
    "test_data, test_label = zip(*c)\n",
    "\n",
    "#print(test_data)\n",
    "#print(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('prep_lalaland.txt') as f:\n",
    "    raw1 = f.read().lower()\n",
    "    \n",
    "with open('prep_godfather.txt') as f:\n",
    "    raw2 = f.read().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = [raw1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = ['lala']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "import re\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "def tokenize_stem(text):\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text)\n",
    "                    for word in nltk.word_tokenize(sent)]\n",
    "    final_tokens = []\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-z]', token):\n",
    "            final_tokens.append(token)\n",
    "\n",
    "    #stems = [stemmer.stem(t) for t in final_tokens]\n",
    "    stems = final_tokens\n",
    "    return stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorizer = TfidfVectorizer(token_pattern=u\"(?u)\\\\b\\\\w+\\\\b\",max_df=0.5, max_features=10,min_df=0.1, stop_words='english',use_idf=True)\n",
    "#vectorizer = TfidfVectorizer(max_features=5000, max_df=0.8, min_df=0.2,stop_words='english', tokenizer=tokenize_stem,use_idf=True)\n",
    "vectorizer = TfidfVectorizer(max_features=5000,stop_words='english', tokenizer=tokenize_stem,use_idf=True)\n",
    "#vectorizer = TfidfVectorizer(max_features=2000, stop_words='english', tokenizer=tokenize_stem)\n",
    "vtr_data = vectorizer.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_tfidf = KNeighborsClassifier(n_neighbors=1, algorithm='brute', metric='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='brute', leaf_size=30, metric='cosine',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=1, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_tfidf.fit(vtr_data, label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['lala']\n",
      "['lala']\n",
      "['lala']\n",
      "['lala']\n",
      "['lala']\n"
     ]
    }
   ],
   "source": [
    "pred_data = ['cafe jazz la holywood', #lala\n",
    "            'The judge sentenced them to three years in prison', #god\n",
    "            'he is weeping now', #god\n",
    "            'CITY OF STARS', #lala\n",
    "             'You ask for too much' #god\n",
    "            ]\n",
    "\n",
    "for pred_sentence in pred_data:\n",
    "    pred_vec = vectorizer.transform([pred_sentence.lower()])\n",
    "    print(knn_tfidf.predict(pred_vec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "2503\n",
      "1.0\n"
     ]
    }
   ],
   "source": [
    "cnt = 0\n",
    "s_cnt = 0\n",
    "for sent, label in zip(test_data, test_label):\n",
    "    pred_sent = ''\n",
    "    s_cnt += 1\n",
    "    for word in sent:\n",
    "        pred_sent += word + ' '\n",
    "    pred_vec = vectorizer.transform([pred_sent.lower()])\n",
    "    pre = knn_tfidf.predict(pred_vec)\n",
    "    \n",
    "    if(label!=pre[0]):\n",
    "        print(pre[0])\n",
    "        print(label)\n",
    "        cnt += 1\n",
    "        \n",
    "print(cnt)\n",
    "print(s_cnt)\n",
    "print((s_cnt - cnt) / s_cnt )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
