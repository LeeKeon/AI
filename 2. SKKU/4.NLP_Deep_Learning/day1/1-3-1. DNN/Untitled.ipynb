{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/etc/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " it:    0 | loss: 8.839 - 0.27s\n",
      "test loss: 8.824\n",
      " it:   50 | loss: 7.046 - 25.45s\n",
      " it:  100 | loss: 6.875 - 27.56s\n",
      " it:  150 | loss: 7.042 - 29.68s\n",
      " it:  200 | loss: 6.880 - 32.03s\n",
      " it:  250 | loss: 6.674 - 34.34s\n",
      "test loss: 6.637\n",
      "32 the\n",
      " it:  300 | loss: 6.480 - 54.18s\n",
      " it:  350 | loss: 6.818 - 56.41s\n",
      " it:  400 | loss: 6.661 - 58.77s\n",
      " it:  450 | loss: 6.467 - 61.09s\n",
      " it:  500 | loss: 6.627 - 63.60s\n",
      "test loss: 6.271\n",
      "873 take\n",
      " it:  550 | loss: 6.584 - 83.50s\n",
      " it:  600 | loss: 6.249 - 85.73s\n",
      " it:  650 | loss: 6.156 - 87.88s\n",
      " it:  700 | loss: 5.743 - 90.20s\n",
      " it:  750 | loss: 6.433 - 92.16s\n",
      "test loss: 6.033\n",
      "32 the\n",
      " it:  800 | loss: 6.407 - 112.28s\n",
      " it:  850 | loss: 6.244 - 114.47s\n",
      " it:  900 | loss: 6.292 - 116.64s\n",
      " it:  950 | loss: 6.391 - 118.93s\n",
      " it: 1000 | loss: 6.239 - 121.38s\n",
      "test loss: 6.018\n",
      "WARNING:tensorflow:From /home/frank/4.NLP_Deep_Learning/day1/1-3-1. DNN/dnn_model.py:72: all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Please use tf.global_variables instead.\n",
      " * model saved at 'models/dnn'\n",
      "32 the\n",
      " it: 1050 | loss: 5.403 - 141.57s\n",
      " it: 1100 | loss: 5.604 - 143.67s\n",
      " it: 1150 | loss: 5.915 - 145.75s\n",
      " it: 1200 | loss: 5.799 - 148.03s\n",
      " it: 1250 | loss: 5.682 - 150.32s\n",
      "test loss: 5.942\n",
      "1824 buy\n",
      " it: 1300 | loss: 5.576 - 168.37s\n",
      " it: 1350 | loss: 5.799 - 170.48s\n",
      " it: 1400 | loss: 5.689 - 172.66s\n",
      " it: 1450 | loss: 5.571 - 174.72s\n",
      " it: 1500 | loss: 5.787 - 177.03s\n",
      "test loss: 5.894\n",
      "873 take\n",
      " it: 1550 | loss: 5.868 - 193.41s\n",
      " it: 1600 | loss: 5.384 - 195.31s\n",
      " it: 1650 | loss: 5.430 - 197.17s\n",
      " it: 1700 | loss: 5.135 - 199.12s\n",
      " it: 1750 | loss: 5.736 - 200.76s\n",
      "test loss: 5.839\n",
      "32 the\n",
      " it: 1800 | loss: 5.745 - 216.47s\n",
      " it: 1850 | loss: 5.499 - 218.36s\n",
      " it: 1900 | loss: 5.603 - 220.25s\n",
      " it: 1950 | loss: 5.822 - 222.17s\n",
      " it: 2000 | loss: 5.607 - 224.23s\n",
      "test loss: 5.787\n",
      " * model saved at 'models/dnn'\n",
      "32 the\n",
      " it: 2050 | loss: 5.076 - 240.18s\n",
      " it: 2100 | loss: 5.290 - 241.96s\n",
      " it: 2150 | loss: 5.690 - 243.79s\n",
      " it: 2200 | loss: 5.452 - 245.78s\n",
      " it: 2250 | loss: 5.292 - 247.69s\n",
      "test loss: 5.784\n",
      "26 <unk>\n",
      " it: 2300 | loss: 5.175 - 263.31s\n",
      " it: 2350 | loss: 5.393 - 265.27s\n",
      " it: 2400 | loss: 5.281 - 267.30s\n",
      " it: 2450 | loss: 5.124 - 269.15s\n",
      " it: 2500 | loss: 5.396 - 271.31s\n",
      "test loss: 5.613\n",
      "26 <unk>\n",
      " it: 2550 | loss: 5.478 - 287.07s\n",
      " it: 2600 | loss: 4.937 - 288.99s\n",
      " it: 2650 | loss: 5.010 - 290.88s\n",
      " it: 2700 | loss: 4.726 - 292.85s\n",
      " it: 2750 | loss: 5.375 - 294.51s\n",
      "test loss: 5.565\n",
      "1248 result\n",
      " it: 2800 | loss: 5.387 - 310.15s\n",
      " it: 2850 | loss: 5.088 - 312.03s\n",
      " it: 2900 | loss: 5.160 - 313.88s\n",
      " it: 2950 | loss: 5.492 - 315.86s\n",
      " it: 3000 | loss: 5.239 - 317.98s\n",
      "test loss: 5.414\n",
      " * model saved at 'models/dnn'\n",
      "1004 pay\n",
      " it: 3050 | loss: 4.888 - 334.07s\n",
      " it: 3100 | loss: 5.083 - 335.90s\n",
      " it: 3150 | loss: 5.415 - 337.71s\n",
      " it: 3200 | loss: 5.112 - 339.64s\n",
      " it: 3250 | loss: 5.005 - 341.49s\n",
      "test loss: 5.450\n",
      "26 <unk>\n",
      " it: 3300 | loss: 4.876 - 356.81s\n",
      " it: 3350 | loss: 5.127 - 358.76s\n",
      " it: 3400 | loss: 5.021 - 360.84s\n",
      " it: 3450 | loss: 4.819 - 362.86s\n",
      " it: 3500 | loss: 5.135 - 365.11s\n",
      "test loss: 5.276\n",
      "26 <unk>\n",
      " it: 3550 | loss: 5.150 - 382.52s\n",
      " it: 3600 | loss: 4.599 - 384.60s\n",
      " it: 3650 | loss: 4.742 - 386.66s\n",
      " it: 3700 | loss: 4.434 - 388.80s\n",
      " it: 3750 | loss: 5.022 - 390.65s\n",
      "test loss: 5.195\n",
      "1248 result\n",
      " it: 3800 | loss: 5.058 - 408.45s\n",
      " it: 3850 | loss: 4.779 - 410.44s\n",
      " it: 3900 | loss: 4.910 - 412.45s\n",
      " it: 3950 | loss: 5.142 - 414.57s\n",
      " it: 4000 | loss: 4.836 - 416.89s\n",
      "test loss: 5.007\n",
      " * model saved at 'models/dnn'\n",
      "1004 pay\n",
      " it: 4050 | loss: 4.591 - 434.78s\n",
      " it: 4100 | loss: 4.705 - 436.67s\n",
      " it: 4150 | loss: 5.139 - 438.64s\n",
      " it: 4200 | loss: 4.758 - 440.74s\n",
      " it: 4250 | loss: 4.714 - 442.80s\n",
      "test loss: 4.897\n",
      "26 <unk>\n",
      " it: 4300 | loss: 4.627 - 460.31s\n",
      " it: 4350 | loss: 4.871 - 462.33s\n",
      " it: 4400 | loss: 4.702 - 464.53s\n",
      " it: 4450 | loss: 4.501 - 466.61s\n",
      " it: 4500 | loss: 4.869 - 468.86s\n",
      "test loss: 4.804\n",
      "26 <unk>\n",
      " it: 4550 | loss: 4.881 - 486.35s\n",
      " it: 4600 | loss: 4.308 - 488.40s\n",
      " it: 4650 | loss: 4.472 - 490.44s\n",
      " it: 4700 | loss: 4.163 - 492.59s\n",
      " it: 4750 | loss: 4.671 - 494.41s\n",
      "test loss: 4.681\n",
      "476 international\n",
      " it: 4800 | loss: 4.736 - 512.01s\n",
      " it: 4850 | loss: 4.458 - 514.00s\n",
      " it: 4900 | loss: 4.552 - 515.96s\n",
      " it: 4950 | loss: 4.792 - 518.04s\n",
      " it: 5000 | loss: 4.474 - 520.26s\n",
      "test loss: 4.539\n",
      " * model saved at 'models/dnn'\n",
      "1004 pay\n",
      " it: 5050 | loss: 4.374 - 537.96s\n",
      " it: 5100 | loss: 4.412 - 539.84s\n",
      " it: 5150 | loss: 4.783 - 541.76s\n",
      " it: 5200 | loss: 4.458 - 543.80s\n",
      " it: 5250 | loss: 4.443 - 545.80s\n",
      "test loss: 4.415\n",
      "409 continue\n",
      " it: 5300 | loss: 4.258 - 561.70s\n",
      " it: 5350 | loss: 4.588 - 563.66s\n",
      " it: 5400 | loss: 4.420 - 565.73s\n",
      " it: 5450 | loss: 4.245 - 567.71s\n",
      " it: 5500 | loss: 4.532 - 569.88s\n",
      "test loss: 4.383\n",
      "873 take\n",
      " it: 5550 | loss: 4.557 - 585.92s\n",
      " it: 5600 | loss: 3.955 - 587.90s\n",
      " it: 5650 | loss: 4.099 - 589.79s\n",
      " it: 5700 | loss: 3.861 - 591.81s\n",
      " it: 5750 | loss: 4.292 - 593.51s\n",
      "test loss: 4.186\n",
      "26 <unk>\n",
      " it: 5800 | loss: 4.343 - 609.42s\n",
      " it: 5850 | loss: 4.036 - 611.33s\n",
      " it: 5900 | loss: 4.166 - 613.22s\n",
      " it: 5950 | loss: 4.475 - 615.23s\n",
      " it: 6000 | loss: 4.126 - 617.37s\n",
      "test loss: 4.069\n",
      " * model saved at 'models/dnn'\n",
      "1773 get\n",
      " it: 6050 | loss: 4.001 - 633.70s\n",
      " it: 6100 | loss: 4.117 - 635.56s\n",
      " it: 6150 | loss: 4.416 - 637.40s\n",
      " it: 6200 | loss: 4.192 - 639.36s\n",
      " it: 6250 | loss: 4.060 - 641.35s\n",
      "test loss: 3.924\n",
      "409 continue\n",
      " it: 6300 | loss: 3.979 - 657.34s\n",
      " it: 6350 | loss: 4.277 - 659.34s\n",
      " it: 6400 | loss: 4.084 - 661.42s\n",
      " it: 6450 | loss: 3.879 - 663.38s\n",
      " it: 6500 | loss: 4.193 - 665.52s\n",
      "test loss: 3.921\n",
      "873 take\n",
      " it: 6550 | loss: 4.226 - 681.66s\n",
      " it: 6600 | loss: 3.576 - 683.61s\n",
      " it: 6650 | loss: 3.761 - 685.46s\n",
      " it: 6700 | loss: 3.596 - 687.46s\n",
      " it: 6750 | loss: 3.915 - 689.12s\n",
      "test loss: 3.714\n",
      "26 <unk>\n",
      " it: 6800 | loss: 4.018 - 704.54s\n",
      " it: 6850 | loss: 3.604 - 706.46s\n",
      " it: 6900 | loss: 3.824 - 708.33s\n",
      " it: 6950 | loss: 4.080 - 710.29s\n",
      " it: 7000 | loss: 3.805 - 712.35s\n",
      "test loss: 3.691\n",
      " * model saved at 'models/dnn'\n",
      "154 have\n",
      " it: 7050 | loss: 3.706 - 727.67s\n",
      " it: 7100 | loss: 3.787 - 729.52s\n",
      " it: 7150 | loss: 4.074 - 731.39s\n",
      " it: 7200 | loss: 3.908 - 733.30s\n",
      " it: 7250 | loss: 3.742 - 735.19s\n",
      "test loss: 3.512\n",
      "409 continue\n",
      " it: 7300 | loss: 3.638 - 750.49s\n",
      " it: 7350 | loss: 3.956 - 752.40s\n",
      " it: 7400 | loss: 3.728 - 754.42s\n",
      " it: 7450 | loss: 3.528 - 756.25s\n",
      " it: 7500 | loss: 3.874 - 758.31s\n",
      "test loss: 3.479\n",
      "873 take\n",
      " it: 7550 | loss: 3.837 - 773.56s\n",
      " it: 7600 | loss: 3.267 - 775.48s\n",
      " it: 7650 | loss: 3.413 - 777.36s\n",
      " it: 7700 | loss: 3.263 - 779.35s\n",
      " it: 7750 | loss: 3.549 - 781.02s\n",
      "test loss: 3.299\n",
      "409 continue\n",
      " it: 7800 | loss: 3.692 - 795.94s\n",
      " it: 7850 | loss: 3.323 - 797.80s\n",
      " it: 7900 | loss: 3.434 - 799.61s\n",
      " it: 7950 | loss: 3.725 - 801.60s\n",
      " it: 8000 | loss: 3.442 - 803.77s\n",
      "test loss: 3.303\n",
      " * model saved at 'models/dnn'\n",
      "409 continue\n",
      " it: 8050 | loss: 3.461 - 819.70s\n",
      " it: 8100 | loss: 3.525 - 821.47s\n",
      " it: 8150 | loss: 3.772 - 823.31s\n",
      " it: 8200 | loss: 3.604 - 825.19s\n",
      " it: 8250 | loss: 3.441 - 827.10s\n",
      "test loss: 3.135\n",
      "409 continue\n",
      " it: 8300 | loss: 3.344 - 842.35s\n",
      " it: 8350 | loss: 3.642 - 844.25s\n",
      " it: 8400 | loss: 3.469 - 846.27s\n",
      " it: 8450 | loss: 3.219 - 848.13s\n",
      " it: 8500 | loss: 3.537 - 850.24s\n",
      "test loss: 3.143\n",
      "1410 major\n",
      " it: 8550 | loss: 3.520 - 865.49s\n",
      " it: 8600 | loss: 2.994 - 867.46s\n",
      " it: 8650 | loss: 3.119 - 869.32s\n",
      " it: 8700 | loss: 2.997 - 871.32s\n",
      " it: 8750 | loss: 3.223 - 873.02s\n",
      "test loss: 3.006\n",
      "409 continue\n",
      " it: 8800 | loss: 3.363 - 887.80s\n",
      " it: 8850 | loss: 3.032 - 889.78s\n",
      " it: 8900 | loss: 3.147 - 891.62s\n",
      " it: 8950 | loss: 3.377 - 893.64s\n",
      " it: 9000 | loss: 3.113 - 895.76s\n",
      "test loss: 3.032\n",
      " * model saved at 'models/dnn'\n",
      "409 continue\n",
      " it: 9050 | loss: 3.193 - 911.30s\n",
      " it: 9100 | loss: 3.191 - 913.08s\n",
      " it: 9150 | loss: 3.507 - 914.93s\n",
      " it: 9200 | loss: 3.342 - 916.88s\n",
      " it: 9250 | loss: 3.153 - 918.77s\n",
      "test loss: 2.828\n",
      "409 continue\n",
      " it: 9300 | loss: 3.084 - 933.81s\n",
      " it: 9350 | loss: 3.396 - 935.72s\n",
      " it: 9400 | loss: 3.137 - 937.79s\n",
      " it: 9450 | loss: 2.958 - 939.73s\n",
      " it: 9500 | loss: 3.281 - 941.84s\n",
      "test loss: 2.836\n",
      "65 make\n",
      " it: 9550 | loss: 3.305 - 957.04s\n",
      " it: 9600 | loss: 2.783 - 958.94s\n",
      " it: 9650 | loss: 2.862 - 960.80s\n",
      " it: 9700 | loss: 2.742 - 962.81s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " it: 9750 | loss: 2.910 - 964.48s\n",
      "test loss: 2.723\n",
      "409 continue\n",
      " it: 9800 | loss: 3.098 - 979.68s\n",
      " it: 9850 | loss: 2.831 - 981.59s\n",
      " it: 9900 | loss: 2.843 - 983.36s\n",
      " it: 9950 | loss: 3.152 - 985.29s\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"3\"\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "from dnn_model import Model\n",
    "from data_loader import text_data\n",
    "data = text_data(\"./dataset/ptb/\")\n",
    "\n",
    "\n",
    "def initialize_session():\n",
    "    config = tf.ConfigProto()\n",
    "    #config.gpu_options.allow_growth = True\n",
    "    config.gpu_options.per_process_gpu_memory_fraction = 0.4\n",
    "    return tf.Session(config=config)\n",
    "\n",
    "##################################################\n",
    "BATCH_SIZE = 5         # 배치 사이즈\n",
    "num_k = 7               # 앞에 볼 단어 개수\n",
    "emb_dim = 64            # 단어 embedding dimension\n",
    "learning_rate = 0.0005  # Learning rate\n",
    "use_clip = True         # Gradient clipping 쓸지 여부\n",
    "##################################################\n",
    "\n",
    "model = Model(num_k=num_k, emb_dim=emb_dim, vocab_size=data.vocab_size,\n",
    "              use_clip=True, learning_rate=learning_rate)\n",
    "\n",
    "sess = initialize_session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "\n",
    "def sample_test(test_input=\"\"):\n",
    "    # test_input = raw_input(\"test text: \") # input(\"test text: \") for python 2, 3\n",
    "    test_x = np.zeros((1, num_k), dtype=np.int32)\n",
    "    words = test_input.split()\n",
    "    for i in range(min(num_k, len(words))):\n",
    "        test_x[0][-i-1] = data.w2idx[words[-i-1]]\n",
    "    out_x = sess.run(model.out_y, feed_dict={model.x: test_x})\n",
    "    print(out_x[0], data.idx2w[out_x[0]])\n",
    "\n",
    "def test_model():\n",
    "    num_it = int(len(data.test_ids) / BATCH_SIZE)\n",
    "    test_x = np.zeros((BATCH_SIZE, num_k), dtype=np.int32)\n",
    "    mask = np.zeros(BATCH_SIZE, dtype=np.int32)\n",
    "    test_loss, test_cnt = 0, 0\n",
    "\n",
    "    for _ in range(num_it):\n",
    "        test_ids, length = data.get_test(BATCH_SIZE)\n",
    "        max_len = max(length)\n",
    "\n",
    "        test_x.fill(0)\n",
    "        mask.fill(0)\n",
    "\n",
    "        for i in range(num_k - 1, max_len - 2):\n",
    "            for batch in range(len(test_ids)):\n",
    "                for j in range(0, num_k):\n",
    "                    if i < j or i - j >= length[batch]:\n",
    "                        break\n",
    "                    test_x[batch][num_k - j - 1] = test_ids[batch][i - j]\n",
    "                mask[batch] = 1 if length[batch] > i+1 else 0\n",
    "                if length[batch] > i + 1:\n",
    "                    input_y[batch] = test_ids[batch][i + 1]\n",
    "\n",
    "            loss = sess.run(model.loss, feed_dict={model.x: test_x, model.y: input_y, model.mask: mask})\n",
    "            test_loss += loss\n",
    "            test_cnt += 1\n",
    "    print(\"test loss: {:.3f}\".format(test_loss / test_cnt))\n",
    "\n",
    "\n",
    "input_x = np.zeros((BATCH_SIZE, num_k), dtype=np.int32)\n",
    "input_y = np.zeros(BATCH_SIZE, dtype=np.int32)\n",
    "input_mask = np.zeros(BATCH_SIZE, dtype=np.int32)\n",
    "length = np.zeros(BATCH_SIZE, dtype=np.int32)\n",
    "\n",
    "avg_loss, it_cnt = 0, 0\n",
    "it_log, it_test, it_save, it_sample = 50, 250, 1000, 250\n",
    "start_time = time.time()\n",
    "\n",
    "for it in range(0, 10000):\n",
    "    train_ids, length = data.get_train(BATCH_SIZE)\n",
    "    max_len = max(length)\n",
    "    input_x.fill(0)\n",
    "\n",
    "    for i in range(num_k-1, max_len-2):\n",
    "        for batch in range(len(train_ids)):\n",
    "            for j in range(0, num_k):\n",
    "                if i < j or i-j >= length[batch]:\n",
    "                    break\n",
    "                input_x[batch][num_k-j-1] = train_ids[batch][i-j]\n",
    "            input_mask[batch] = 1 if length[batch] > i+1 else 0\n",
    "\n",
    "            if length[batch] > i+1:\n",
    "                input_y[batch] = train_ids[batch][i+1]\n",
    "\n",
    "        loss, _ = sess.run([model.loss, model.update],\n",
    "                           feed_dict={model.x: input_x, model.y: input_y, model.mask: input_mask})\n",
    "        avg_loss += loss\n",
    "        it_cnt += 1\n",
    "\n",
    "    if it % it_log == 0:\n",
    "        print(\" it: {:4d} | loss: {:.3f} - {:.2f}s\".format(it, avg_loss / it_cnt, time.time() - start_time))\n",
    "        avg_loss, it_cnt = 0, 0\n",
    "\n",
    "    if it % it_test == 0:\n",
    "        test_model()\n",
    "    if it % it_save == 0 and it > 0:\n",
    "        model.save(sess)\n",
    "    if it % it_sample == 0 and it > 0:\n",
    "        sample_test(test_input=\"again the specialists were not able to\")\n",
    "\n",
    "sess.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
