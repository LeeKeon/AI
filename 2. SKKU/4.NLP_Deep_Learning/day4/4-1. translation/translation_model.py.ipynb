{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/etc/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.layers.core import Dense\n",
    "\n",
    "from tensorflow.contrib.rnn import LSTMCell, LSTMStateTuple\n",
    "from tensorflow.python.ops.rnn import bidirectional_dynamic_rnn, dynamic_rnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your code here\n",
    "class Model(object):\n",
    "    def __init__(self, emb_dim=128, hidden_dim=128, attn_dim=256,\n",
    "                 max_enc_len=50, max_dec_len=50,\n",
    "                 enc_vocab=5000, dec_vocab=5000,\n",
    "                 stt_idx=1, end_idx=2,\n",
    "                 use_clip=True, learning_rate=0.001):\n",
    "        self.initializer = tf.random_uniform_initializer(-0.1, 0.1)\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.attn_dim = attn_dim\n",
    "\n",
    "        self.max_enc_len = max_enc_len\n",
    "        self.max_dec_len = max_dec_len\n",
    "\n",
    "        self.enc_vocab = enc_vocab\n",
    "        self.dec_vocab = dec_vocab\n",
    "\n",
    "        self.stt_idx = stt_idx\n",
    "        self.end_idx = end_idx\n",
    "\n",
    "        self.use_clip = use_clip\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "        # Placeholder\n",
    "        self.x = tf.placeholder(dtype=tf.int32, shape=(None, max_enc_len))\n",
    "        self.x_len = tf.placeholder(dtype=tf.int32, shape=(None, ))\n",
    "\n",
    "        self.y = tf.placeholder(dtype=tf.int32, shape=(None, max_dec_len))\n",
    "        self.y_len = tf.placeholder(dtype=tf.int32, shape=(None, ))\n",
    "\n",
    "        # sequence mask for different size\n",
    "        self.batch_size = tf.shape(self.x)[0]\n",
    "        self.masks = tf.sequence_mask(lengths=self.y_len, maxlen=self.max_dec_len, dtype=tf.float32)\n",
    "\n",
    "        # Embedding\n",
    "        self.emb_W_enc = tf.get_variable(name='emb_W_enc', shape=[self.enc_vocab, self.emb_dim], dtype=tf.float32, initializer=self.initializer)\n",
    "        self.emb_W_dec = tf.get_variable(name='emb_W_dec', shape=[self.dec_vocab, self.emb_dim], dtype=tf.float32, initializer=self.initializer)\n",
    "\n",
    "        self.x_emb = tf.nn.embedding_lookup(self.emb_W_enc, self.x)\n",
    "        self.y_emb = tf.nn.embedding_lookup(self.emb_W_dec, self.y)\n",
    "\n",
    "        self.rnn_encode()\n",
    "        #self.init_enc_attention()\n",
    "\n",
    "        self.decoder_cell = LSTMCell(self.hidden_dim, state_is_tuple=True)\n",
    "        self.out_layer = Dense(self.dec_vocab, name='out_layer')\n",
    "        self.decoder_train()\n",
    "        self.decoder_infer()\n",
    "\n",
    "        self.build_loss()\n",
    "        self.build_opt()\n",
    "\n",
    "    def rnn_encode(self):\n",
    "        # Bi-direction rnn encoder (forward, backward)\n",
    "        (self.enc_out, (fw_state, bw_state)) = bidirectional_dynamic_rnn(\n",
    "            LSTMCell(self.hidden_dim), LSTMCell(self.hidden_dim),\n",
    "            inputs=self.x_emb, sequence_length=self.x_len, dtype=tf.float32)\n",
    "        self.enc_out = tf.concat(self.enc_out, 2)\n",
    "\n",
    "        # Init for decoder's first state\n",
    "        decoder_init_c = Dense(self.hidden_dim, name=\"decoder_c\", activation=tf.nn.tanh, bias_initializer=self.initializer)\n",
    "        decoder_init_h = Dense(self.hidden_dim, name=\"decoder_h\", activation=tf.nn.tanh, bias_initializer=self.initializer)\n",
    "        self.init_state = LSTMStateTuple(decoder_init_c(tf.concat([fw_state.c, bw_state.c], 1)),\n",
    "                                         decoder_init_h(tf.concat([fw_state.h, bw_state.h], 1)))\n",
    "\n",
    "    # 논문: https://arxiv.org/pdf/1409.0473.pdf\n",
    "    # Attention process: softmax(e_t)\n",
    "    # e_t,i = v^T * tanh (U*h_i + W*s_t + b)  --> h_i, s_t 는 각각 encoder / decoder state\n",
    "    # U*h_i + b 는 encoding 만 완료되면 구할 수 있으므로, 미리 구해주고\n",
    "    # decoder state 는 매 디코딩 과정에서 나오는 state를 사용\n",
    "    # encoder 각각의 hidden state에 matrix를 개별적을 곱하기 위해 convolution 연산 사용\n",
    "    def init_enc_attention(self):\n",
    "        # self.attn_dim = 300   # defined at init\n",
    "        self.attUe = tf.get_variable(name='attnU_et', shape=[1, 1, self.hidden_dim * 2, self.attn_dim], initializer=self.initializer)\n",
    "        self.attbe = tf.get_variable(name='attnB_et', shape=[self.attn_dim], initializer=self.initializer)\n",
    "\n",
    "        att_e = tf.nn.conv2d(tf.reshape(self.enc_out, [-1, self.max_enc_len, 1, self.hidden_dim * 2]),\n",
    "                             filter=self.attUe, strides=[1, 1, 1, 1], padding='VALID')\n",
    "        self.attUeh = tf.reshape(att_e + self.attbe, [-1, self.attn_dim])\n",
    "\n",
    "        self.attWe = tf.get_variable(name='attnW_et', shape=[1, 1, self.hidden_dim, self.attn_dim], initializer=self.initializer)\n",
    "        self.attve = tf.get_variable(name='attnV_et', shape=[self.attn_dim, 1], initializer=self.initializer)\n",
    "\n",
    "\n",
    "    # Encoder attention softmax\n",
    "    # decoder state 를 넘겨받아서 attention distribution softmax(e_t) 리턴\n",
    "    # tile: encoder 각 state의 e_t,i 구할 때 decoder state 가 필요하므로 encoder 길이(step)만큼 tiling 해줌\n",
    "    # tiling 한 뒤 마찬가지로 conv 연산 사용 (W 연산 후 tiling해도 가능할 수 있으나, 직관적으로 위와 똑같이 사용)\n",
    "    def encoder_attention(self, state):\n",
    "        # decoder state\n",
    "        st = tf.reshape(tf.tile(state, [1, self.max_enc_len]),\n",
    "                        [-1, self.max_enc_len, 1, self.hidden_dim])\n",
    "        attWes = tf.reshape(tf.nn.conv2d(st, filter=self.attWe, strides=[1, 1, 1, 1], padding='SAME'),\n",
    "                            [-1, self.attn_dim])\n",
    "\n",
    "        # encoder attention distribution\n",
    "        e_t = tf.reshape(tf.matmul(tf.nn.tanh(attWes + self.attUeh), self.attve),\n",
    "                         [self.batch_size, self.max_enc_len])\n",
    "\n",
    "        return tf.nn.softmax(e_t)\n",
    "\n",
    "\n",
    "    def decoder_train(self):\n",
    "        # time-batch-dimension\n",
    "        y_emb_tbd = tf.transpose(self.y_emb, [1, 0, 2])\n",
    "        word_prob = tf.TensorArray(dtype=tf.float32, size=self.max_dec_len)\n",
    "\n",
    "        def body(step, state, word_prob):\n",
    "            enc_softmax = self.encoder_attention(state.h)\n",
    "            #context_vector = tf.reduce_sum(self.enc_out * tf.expand_dims(enc_softmax, -1), axis=1)\n",
    "            context_vector = tf.reduce_sum(self.enc_out, axis=1)\n",
    "\n",
    "            word_logit = self.out_layer(tf.concat([state.h, context_vector], 1))\n",
    "            word_prob = word_prob.write(step, word_logit)\n",
    "\n",
    "            token_emb = y_emb_tbd[step]\n",
    "            inp = tf.concat([token_emb, context_vector], 1)\n",
    "            next_out, next_state = self.decoder_cell(inp, state)\n",
    "\n",
    "            return step + 1, next_state, word_prob\n",
    "\n",
    "        _step, _state, _word_prob = tf.while_loop(\n",
    "            cond=lambda t, _state, _word_prob: t < self.max_dec_len,\n",
    "            body=body,\n",
    "            loop_vars=(0, self.init_state, word_prob))\n",
    "\n",
    "        self.train_prob = tf.transpose(_word_prob.stack(), perm=[1, 0, 2])\n",
    "\n",
    "    def decoder_infer(self):\n",
    "        word_token = tf.TensorArray(dtype=tf.int32, size=self.max_dec_len)\n",
    "\n",
    "        def body(step, state, word_token):\n",
    "            enc_softmax = self.encoder_attention(state.h)\n",
    "            context_vector = tf.reduce_sum(self.enc_out * tf.expand_dims(enc_softmax, -1), axis=1)\n",
    "\n",
    "            word_logit = self.out_layer(tf.concat([state.h, context_vector], 1))\n",
    "            next_token = tf.cast(tf.reshape(tf.argmax(word_logit, 1), [self.batch_size]), tf.int32)\n",
    "            word_token = word_token.write(step, next_token)\n",
    "\n",
    "            token_emb = tf.nn.embedding_lookup(self.emb_W_dec, next_token)\n",
    "            inp = tf.concat([token_emb, context_vector], 1)\n",
    "            next_out, next_state = self.decoder_cell(inp, state)\n",
    "\n",
    "            return step + 1, next_state, word_token\n",
    "\n",
    "        _step, _state, _word_token = tf.while_loop(\n",
    "            cond=lambda t, _state, _word_token: t < self.max_dec_len,\n",
    "            body=body,\n",
    "            loop_vars=(0, self.init_state, word_token))\n",
    "\n",
    "        self.output_token = tf.transpose(_word_token.stack(), perm=[1, 0])\n",
    "\n",
    "    def build_loss(self):\n",
    "        self.cross_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=self.y, logits=self.train_prob)\n",
    "        self.loss = tf.reduce_sum(self.cross_entropy * self.masks) / (tf.reduce_sum(self.masks) + 1e-10)\n",
    "\n",
    "    def build_opt(self):\n",
    "        # define optimizer\n",
    "        optimizer = tf.train.AdamOptimizer(learning_rate=self.learning_rate)\n",
    "        grad, var = zip(*optimizer.compute_gradients(self.loss))\n",
    "\n",
    "        # gradient clipping\n",
    "        def clipped_grad(grad):\n",
    "            return [None if g is None else tf.clip_by_norm(g, 2.5) for g in grad]\n",
    "\n",
    "        if self.use_clip:\n",
    "            grad = clipped_grad(grad)\n",
    "\n",
    "        self.update = optimizer.apply_gradients(zip(grad, var))\n",
    "\n",
    "\n",
    "    def save(self, sess, global_step=None):\n",
    "        var_list = [var for var in tf.all_variables()]\n",
    "        saver = tf.train.Saver(var_list)\n",
    "        save_path = saver.save(sess, save_path=\"models/model\", global_step=global_step)\n",
    "        print(' * model saved at \\'{}\\''.format(save_path))\n",
    "\n",
    "    # Load whole weights\n",
    "    def restore(self, sess):\n",
    "        print(' - Restoring variables...')\n",
    "        var_list = [var for var in tf.all_variables()]\n",
    "        saver = tf.train.Saver(var_list)\n",
    "        saver.restore(sess, \"models/model\")\n",
    "        print(' * model restored ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
